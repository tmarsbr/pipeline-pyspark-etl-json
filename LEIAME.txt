# PySpark e Apache Kafka Para Processamento de Dados em Batch e Streaming
# Preparação do Ambiente de Trabalho com Python e PySpark
# Configuração do Cluster PySpark

# Criar e Inicializar o Cluster
docker-compose -f docker-compose.yml up -d --scale spark-worker=2

# Spark Master
http://localhost:9091

# History Server
http://localhost:18081

# Solução de Problemas Comuns:
# 1. Se houver erro de download do Spark, verifique se a URL no Dockerfile está correta
# 2. URLs alternativas para download do Spark 3.5.0:
#    - https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
#    - https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
# 3. Remova a linha 'version:' do docker-compose.yml se presente